{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Heart Disease by using Neural Networks.\n",
    "\n",
    "**About Dataset**\n",
    "\n",
    "The dataset contains the following features:\n",
    "\n",
    "- age(in years)\n",
    "- sex: (1 = male; 0 = female)\n",
    "- cp: chest pain type\n",
    "- trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n",
    "- chol: serum cholestoral in mg/dl\n",
    "- fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "- restecg: resting electrocardiographic results\n",
    "- thalach: maximum heart rate achieved\n",
    "- exang: exercise induced angina (1 = yes; 0 = no)\n",
    "- oldpeak: ST depression induced by exercise relative to rest\n",
    "- slope: the slope of the peak exercise ST segment\n",
    "- ca: number of major vessels (0-3) colored by flourosopy\n",
    "- thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n",
    "- target: 1 or 0\n",
    "\n",
    "#### To download the dataset<a href=\"https://drive.google.com/file/d/1R5SjStkUsgTgyoAjC_14v13siYh8AAF3/view?usp=sharing\" title=\"Google Drive\"> Click here </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import necessary libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Perform statistical analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
      "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
      "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
      "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
      "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
      "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
      "\n",
      "   ca  thal  target  \n",
      "0   0     1       1  \n",
      "1   0     2       1  \n",
      "2   0     2       1  \n",
      "3   0     2       1  \n",
      "4   0     2       1  \n"
     ]
    }
   ],
   "source": [
    "# Print the first few rows of the dataframe\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              age         sex          cp    trestbps        chol         fbs  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
      "mean    54.366337    0.683168    0.966997  131.623762  246.264026    0.148515   \n",
      "std      9.082101    0.466011    1.032052   17.538143   51.830751    0.356198   \n",
      "min     29.000000    0.000000    0.000000   94.000000  126.000000    0.000000   \n",
      "25%     47.500000    0.000000    0.000000  120.000000  211.000000    0.000000   \n",
      "50%     55.000000    1.000000    1.000000  130.000000  240.000000    0.000000   \n",
      "75%     61.000000    1.000000    2.000000  140.000000  274.500000    0.000000   \n",
      "max     77.000000    1.000000    3.000000  200.000000  564.000000    1.000000   \n",
      "\n",
      "          restecg     thalach       exang     oldpeak       slope          ca  \\\n",
      "count  303.000000  303.000000  303.000000  303.000000  303.000000  303.000000   \n",
      "mean     0.528053  149.646865    0.326733    1.039604    1.399340    0.729373   \n",
      "std      0.525860   22.905161    0.469794    1.161075    0.616226    1.022606   \n",
      "min      0.000000   71.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000  133.500000    0.000000    0.000000    1.000000    0.000000   \n",
      "50%      1.000000  153.000000    0.000000    0.800000    1.000000    0.000000   \n",
      "75%      1.000000  166.000000    1.000000    1.600000    2.000000    1.000000   \n",
      "max      2.000000  202.000000    1.000000    6.200000    2.000000    4.000000   \n",
      "\n",
      "             thal      target  \n",
      "count  303.000000  303.000000  \n",
      "mean     2.313531    0.544554  \n",
      "std      0.612277    0.498835  \n",
      "min      0.000000    0.000000  \n",
      "25%      2.000000    0.000000  \n",
      "50%      2.000000    1.000000  \n",
      "75%      3.000000    1.000000  \n",
      "max      3.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Get the summary statistics of the dataframe\n",
    "print(df.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trestbps    0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "ca          0\n",
      "thal        0\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into features (X) and target (y)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data using StandardScaler\n",
    "\n",
    "# Create StandardScaler object\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data\n",
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit testing data\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Build and train the neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential neural network model\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add input layer with 13 neurons\n",
    "model.add(Dense(13, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hidden layer with 64 neurons\n",
    "\n",
    "model.add(Dense(64, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layer with one neuron\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.6666 - loss: 0.6030 - val_accuracy: 0.7049 - val_loss: 0.5806\n",
      "Epoch 2/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7021 - loss: 0.5720 - val_accuracy: 0.7705 - val_loss: 0.5476\n",
      "Epoch 3/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7848 - loss: 0.5214 - val_accuracy: 0.7869 - val_loss: 0.5176\n",
      "Epoch 4/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8047 - loss: 0.4947 - val_accuracy: 0.8033 - val_loss: 0.4904\n",
      "Epoch 5/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7977 - loss: 0.4785 - val_accuracy: 0.8197 - val_loss: 0.4664\n",
      "Epoch 6/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.7964 - loss: 0.4387 - val_accuracy: 0.8361 - val_loss: 0.4447\n",
      "Epoch 7/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8254 - loss: 0.4203 - val_accuracy: 0.8361 - val_loss: 0.4277\n",
      "Epoch 8/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.8011 - loss: 0.4188 - val_accuracy: 0.8361 - val_loss: 0.4145\n",
      "Epoch 9/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8239 - loss: 0.3973 - val_accuracy: 0.8361 - val_loss: 0.4025\n",
      "Epoch 10/10\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8149 - loss: 0.3908 - val_accuracy: 0.8361 - val_loss: 0.3924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25848465f10>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs= 10, batch_size=32, validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.8178 - loss: 0.4192\n",
      "Test accuracy:  0.836066\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model on testing data\n",
    "loss,accuracy=model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy: 2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the testing data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to binary labels\n",
    "y_pred_binary = (y_pred > 0.5).astype('int32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.76      0.81        29\n",
      "           1       0.81      0.91      0.85        32\n",
      "\n",
      "    accuracy                           0.84        61\n",
      "   macro avg       0.84      0.83      0.83        61\n",
      "weighted avg       0.84      0.84      0.83        61\n",
      "\n",
      "[[22  7]\n",
      " [ 3 29]]\n"
     ]
    }
   ],
   "source": [
    "# Print the classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_test, y_pred_binary))\n",
    "print(confusion_matrix(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this project, we developed a neural network model to predict heart disease using a dataset of clinical features. Our model achieved a test accuracy of 84%, indicating that it is able to correctly predict the presence or absence of heart disease in approximately 84% of the test cases.\n",
    "\n",
    "The results of this project demonstrate the potential of machine learning algorithms to support diagnosis and treatment decisions in clinical settings. However, further research is needed to improve the accuracy and robustness of the model, as well as to explore its applications in real-world clinical practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPROVING RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = [\n",
    "    ('logistic_regression', LogisticRegression()),\n",
    "    ('decision_tree', DecisionTreeClassifier()),\n",
    "    ('random_forest', RandomForestClassifier()),\n",
    "    ('svm', SVC())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'logistic_regression': {'C': [0.1, 1, 10]},\n",
    "    'decision_tree': {'max_depth': [3, 5, 10]},\n",
    "    'random_forest': {'n_estimators': [10, 50, 100]},\n",
    "    'svm': {'C': [0.1, 1, 10]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for logistic_regression: {'C': 1}\n",
      "Best accuracy for logistic_regression: 0.8180272108843537\n",
      "Best parameters for decision_tree: {'max_depth': 3}\n",
      "Best accuracy for decision_tree: 0.7685374149659864\n",
      "Best parameters for random_forest: {'n_estimators': 50}\n",
      "Best accuracy for random_forest: 0.7975340136054422\n",
      "Best parameters for svm: {'C': 1}\n",
      "Best accuracy for svm: 0.8224489795918368\n"
     ]
    }
   ],
   "source": [
    "# Perform grid search\n",
    "for name, model in models:\n",
    "    grid_search = GridSearchCV(model, param_grid[name], cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(f'Best parameters for {name}: {grid_search.best_params_}')\n",
    "    print(f'Best accuracy for {name}: {grid_search.best_score_}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for logistic_regression: 0.8524590163934426\n",
      "Classification report for logistic_regression:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.85        29\n",
      "           1       0.87      0.84      0.86        32\n",
      "\n",
      "    accuracy                           0.85        61\n",
      "   macro avg       0.85      0.85      0.85        61\n",
      "weighted avg       0.85      0.85      0.85        61\n",
      "\n",
      "Confusion matrix for logistic_regression:\n",
      "[[25  4]\n",
      " [ 5 27]]\n",
      "Accuracy for decision_tree: 0.819672131147541\n",
      "Classification report for decision_tree:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.86      0.82        29\n",
      "           1       0.86      0.78      0.82        32\n",
      "\n",
      "    accuracy                           0.82        61\n",
      "   macro avg       0.82      0.82      0.82        61\n",
      "weighted avg       0.82      0.82      0.82        61\n",
      "\n",
      "Confusion matrix for decision_tree:\n",
      "[[25  4]\n",
      " [ 7 25]]\n",
      "Accuracy for random_forest: 0.8688524590163934\n",
      "Classification report for random_forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        29\n",
      "           1       0.88      0.88      0.88        32\n",
      "\n",
      "    accuracy                           0.87        61\n",
      "   macro avg       0.87      0.87      0.87        61\n",
      "weighted avg       0.87      0.87      0.87        61\n",
      "\n",
      "Confusion matrix for random_forest:\n",
      "[[25  4]\n",
      " [ 4 28]]\n",
      "Accuracy for svm: 0.8688524590163934\n",
      "Classification report for svm:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.90      0.87        29\n",
      "           1       0.90      0.84      0.87        32\n",
      "\n",
      "    accuracy                           0.87        61\n",
      "   macro avg       0.87      0.87      0.87        61\n",
      "weighted avg       0.87      0.87      0.87        61\n",
      "\n",
      "Confusion matrix for svm:\n",
      "[[26  3]\n",
      " [ 5 27]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the models\n",
    "for name, model in models:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f'Accuracy for {name}: {accuracy_score(y_test, y_pred)}')\n",
    "    print(f'Classification report for {name}:')\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f'Confusion matrix for {name}:')\n",
    "    print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Results\n",
    "\n",
    "By using Logistic Regression, Decision Tree classifier and Random forest classifier, we got the accuracy to be 85.2%, 81.9% and 86.8% respectively.\n",
    "While neural networks are powerful models that can learn complex patterns in data, they may not always perform better than logistic regression and decision tree classifier and random forest classifier. This depends aon the quality of the dataset and the methods which are followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
